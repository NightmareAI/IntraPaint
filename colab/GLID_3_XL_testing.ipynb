{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLID-3-XL testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNsBpvcC9vITvMI4V2rvM9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/centuryglass/glid-3-xl-expanded-inpainting/blob/master/colab/GLID_3_XL_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLID-3-XL Inpainting/Image Generation\n",
        "\n",
        "Run all steps in order without changes to test inpainting.  Update the variables in the first two steps to alter inpainting/image generation to fit your needs. Not all options are fully tested, but basic inpainting and image generation should work.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Define image generation variables:\n",
        "If you've already run every step in this notebook, you can change these values, then immediately re-run the image generation step to get new results \n",
        "with the updated settings:"
      ],
      "metadata": {
        "id": "Oqa2CUxXJEsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image to use for inpainting:\n",
        "edit='http://images.wikia.com/dukenukem/images/c/ca/256x256.jpg'\n",
        "\n",
        "# Image mask covering the areas that should be edited:\n",
        "mask='https://i.imgur.com/nSLxIv5.png' \n",
        "\n",
        "# Inpainted areas should fit this description:\n",
        "prompt='galaxy brain'\n",
        "\n",
        "# Inpainted areas should not fit this description:\n",
        "negative='realistic anatomy'\n",
        "\n",
        "# batch_size*num_batches inpainting sample images will be generated:\n",
        "batch_size=3\n",
        "num_batches=3\n",
        "\n",
        "# init_image doesn't work with inpainting, leave as None unless you're going\n",
        "# to update the notebook to use the standard image generation model:\n",
        "init_image=None\n",
        "\n",
        "# Image dimensions:\n",
        "# Values other than 256,256 are unlikely to work well, especially for\n",
        "# inpainting:\n",
        "width=256\n",
        "height=256\n",
        "\n",
        "# Number of diffusion steps (defaults to 27 if set to None):\n",
        "steps=None\n",
        "\n",
        "# Higher values of guidance_scale adhere more closely to the prompt, but are\n",
        "# less likely to produce interesting variations:\n",
        "guidance_scale=5.0\n"
      ],
      "metadata": {
        "id": "im2LBiKAXdVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model init settings:\n",
        "If you change any of these values, you will need to re-run the \n",
        "\"load all models\" step (and possibly the \"download required models\" step)\n",
        "to apply changes."
      ],
      "metadata": {
        "id": "PWZmSGiiKV57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Name of the primary model to use:\n",
        "model_path = 'inpaint.pt'\n",
        "# \"finetune.pt\" and \"diffusion.pt\" are also valid options. Those models are for\n",
        "# regular image generation, not inpainting, so 'edit' and 'mask' will be unset\n",
        "# if these models are used.\n",
        "if model_path != 'inpaint.pt':\n",
        "  edit=None\n",
        "  mask=None\n",
        "\n",
        "# Set to true to load models from your google drive instead of downloading:\n",
        "use_google_drive=False\n",
        "\n",
        "# Clip model to use. options are RN50, RN101, RN50x4, RN50x16, RN50x64,\n",
        "#  ViT-B/32, ViT-B/16, ViT-L/14. Most of these are untested.\n",
        "clip_model_name = 'ViT-L/14'\n",
        "\n",
        "# Set clip_guidance to true for greater accuracy but reduced speed.\n",
        "# This probably requires Colab Pro.\n",
        "clip_guidance=False\n",
        "clip_guidance_scale=150\n",
        "\n",
        "# Changing these will subtly alter the image generation process in ways I\n",
        "# haven't bothered to identify yet. You should just be able to leave them as-is\n",
        "# and still get good results:\n",
        "cutn=16\n",
        "ddim=False\n",
        "ddpm=False # Not working currently, leave this as False for now"
      ],
      "metadata": {
        "id": "D9GdtsHq494q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Initial loading steps:\n",
        "Unless you change model settings or restart the session, you should only need to run these steps once."
      ],
      "metadata": {
        "id": "1s59iiwaKfLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies:\n",
        "!pip install ipywidgets omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops ftfy regex tqdm transformers\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n",
        "!git clone https://github.com/CompVis/latent-diffusion.git\n",
        "!pip install -e taming-transformers\n",
        "!pip install -e latent-diffusion\n",
        "import sys\n",
        "sys.path.append('/content/taming-transformers')\n",
        "sys.path.append('/content/latent-diffusion')\n",
        "!git clone https://github.com/centuryglass/glid-3-xl-expanded-inpainting\n",
        "%cd glid-3-xl-expanded-inpainting\n",
        "!git fetch origin\n",
        "!git checkout origin/colab-refactor\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "id": "frCfhXDtegZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download required models:\n",
        "if use_google_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !cp /content/gdrive/MyDrive/bert.pt .\n",
        "    !cp /content/gdrive/MyDrive/kl-f8.pt .\n",
        "    !cp '/content/gdrive/MyDrive/{model_path}' .\n",
        "else:\n",
        "    !wget https://dall-3.com/models/glid-3-xl/bert.pt\n",
        "    !wget https://dall-3.com/models/glid-3-xl/kl-f8.pt\n",
        "    !wget 'https://dall-3.com/models/glid-3-xl/{model_path}'"
      ],
      "metadata": {
        "id": "G-btQS-uzdTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all models: \n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "device = torch.device('cuda:0')\n",
        "from startup.load_models import loadModels\n",
        "model_params, model, diffusion, ldm, bert, clip_model, clip_preprocess, normalize = loadModels(\n",
        "    device,\n",
        "    model_path=model_path,\n",
        "    clip_model_name=clip_model_name,\n",
        "    steps=steps,\n",
        "    clip_guidance=clip_guidance,\n",
        "    ddpm=ddpm,\n",
        "    ddim=ddim)"
      ],
      "metadata": {
        "id": "-cg422vV1x1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generation:\n",
        "You can re-run this step as many times as you want to continue producing new images. Each run will overwrite the images from the previous run, so make sure to download any that you want to keep first."
      ],
      "metadata": {
        "id": "O72RFtYpK4wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare sample generation function, generate images:\n",
        "from startup.create_sample_function import createSampleFunction\n",
        "sample_fn = createSampleFunction(\n",
        "        device,\n",
        "        model,\n",
        "        model_params,\n",
        "        bert,\n",
        "        clip_model,\n",
        "        ldm,\n",
        "        diffusion,\n",
        "        edit=edit,\n",
        "        mask=mask,\n",
        "        prompt=prompt,\n",
        "        negative=negative,\n",
        "        guidance_scale=guidance_scale,\n",
        "        batch_size=batch_size,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        cutn=cutn,\n",
        "        edit_width=256,\n",
        "        edit_height=256,\n",
        "        edit_x=0,\n",
        "        edit_y=0,\n",
        "        clip_guidance=clip_guidance,\n",
        "        clip_guidance_scale=clip_guidance_scale,\n",
        "        skip_timesteps=0,\n",
        "        ddpm=ddpm,\n",
        "        ddim=ddim)  \n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision.transforms import functional as TF\n",
        "import numpy as np\n",
        "from startup.utils import *\n",
        "from startup.generate_samples import generateSamples\n",
        "!rm -rf output output_npy\n",
        "!mkdir output output_npy\n",
        "def save_sample(i, sample, clip_score=False):\n",
        "    for k, image in enumerate(sample['pred_xstart'][:batch_size]):\n",
        "        image /= 0.18215\n",
        "        im = image.unsqueeze(0)\n",
        "        out = ldm.decode(im)\n",
        "\n",
        "        npy_filename = f'output_npy/{i * batch_size + k:05}.npy'\n",
        "        with open(npy_filename, 'wb') as outfile:\n",
        "            np.save(outfile, image.detach().cpu().numpy())\n",
        "\n",
        "        out = TF.to_pil_image(out.squeeze(0).add(1).div(2).clamp(0, 1))\n",
        "\n",
        "        filename = f'output/{i * batch_size + k:05}.png'\n",
        "        out.save(filename)\n",
        "\n",
        "        if clip_score:\n",
        "          import clip\n",
        "          text = clip.tokenize([text]*batch_size, truncate=True).to(device)\n",
        "          text_clip_blank = clip.tokenize([negative]*batch_size, truncate=True).to(device)\n",
        "          # clip context\n",
        "          text_emb_clip = clip_model.encode_text(text)\n",
        "          text_emb_clip_blank = clip_model.encode_text(text_clip_blank)\n",
        "          image_emb = clip_model.encode_image(clip_preprocess(out).unsqueeze(0).to(device))\n",
        "          image_emb_norm = image_emb / image_emb.norm(dim=-1, keepdim=True)\n",
        "          text_emb_norm = text_emb_clip[0] / text_emb_clip[0].norm(dim=-1, keepdim=True)\n",
        "          similarity = torch.nn.functional.cosine_similarity(image_emb_norm, text_emb_norm, dim=-1)\n",
        "\n",
        "          final_filename = f'output/{similarity.item():0.3f}_{i * batch_size + k:05}.png'\n",
        "          os.rename(filename, final_filename)\n",
        "\n",
        "          npy_final = f'output_npy/{similarity.item():0.3f}_{i * batch_size + k:05}.npy'\n",
        "          os.rename(npy_filename, npy_final)\n",
        "gc.collect()\n",
        "generateSamples(ldm, diffusion, sample_fn, save_sample, batch_size, num_batches)\n",
        "\n",
        "# View results:\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "for sample in os.listdir('output'):\n",
        "  img = cv2.imread(f'output/{sample}', cv2.IMREAD_UNCHANGED)\n",
        "  cv2_imshow(img)"
      ],
      "metadata": {
        "id": "5MSOV77VYrMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: run this to download all results as a zip file.\n",
        "!zip -r results.zip output\n",
        "from google.colab import files\n",
        "files.download(\"results.zip\")"
      ],
      "metadata": {
        "id": "Cx8LqFyWRmH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}